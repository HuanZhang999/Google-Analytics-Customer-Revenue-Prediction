{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from dataprocess_lib0 import *\n",
    "from IPython.core.pylabtools import figsize\n",
    "import gc, json\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tr(csv_path, nrows=None, skiprows=None):\n",
    "    \n",
    "    usecols = ['channelGrouping', 'date', 'device',\n",
    "       'fullVisitorId', 'geoNetwork', 'totals',\n",
    "       'trafficSource', 'visitId', 'visitNumber', 'visitStartTime' ]\n",
    "    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    ans = pd.DataFrame()\n",
    "    trs = pd.read_csv(csv_path, \n",
    "            sep=',',\n",
    "            usecols = usecols,\n",
    "            converters={column: json.loads for column in json_cols}, \n",
    "            dtype={'fullVisitorId': 'str',\n",
    "                  'channelGrouping': 'str',                 \n",
    "                  'visitId':'int',\n",
    "                  'visitNumber':'int',\n",
    "                  'visitStartTime':'int'}, \n",
    "            parse_dates=['date'], \n",
    "            chunksize=100000,\n",
    "            nrows=nrows,\n",
    "            skiprows=skiprows)\n",
    "    \n",
    "    for tr in trs:\n",
    "        tr.reset_index(drop=True, inplace=True)\n",
    "        for column in json_cols:\n",
    "            column_as_tr = json_normalize(tr[column])\n",
    "            column_as_tr.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_tr.columns]\n",
    "            tr = tr.drop(column, axis=1).merge(column_as_tr, right_index=True, left_index=True)\n",
    "\n",
    "        print(f\"Loaded {os.path.basename(csv_path)}. Shape: {tr.shape}\")\n",
    "        tr_chunk = tr  #[features]\n",
    "        del tr\n",
    "        gc.collect()\n",
    "        ans = pd.concat([ans, tr_chunk], axis=0).reset_index(drop=True)\n",
    "        #print(ans.shape)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Data Cleaning\n",
    "* load data (js-type data and data types)\n",
    "* target column\n",
    "* date \n",
    "* drop duplicated and one-unique value columns\n",
    "* Missing and optimal filling values (category data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=Path('../data/Google Analytics Customer Revenue Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tr = load_tr(PATH/'train_v2.csv')\n",
    "print('train date:', min(tr['date']), 'to', max(tr['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = load_tr(PATH/'test_v2.csv')\n",
    "print('test date:', min(te['date']), 'to', max(te['date']))\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "tr0 = pd.concat([tr, te], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr0\n",
    "#tr = tr0.sample(n=11000, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr[\"date\"] = pd.to_datetime(tr[\"date\"], infer_datetime_format=True, format=\"%Y%m%d\")\n",
    "tr['totals_hits'] = tr['totals_hits'].astype(float)\n",
    "tr['totals_pageviews'] = tr['totals_pageviews'].astype(float)\n",
    "tr['totals_timeOnSite'] = tr['totals_timeOnSite'].astype(float)\n",
    "tr['totals_newVisits'] = tr['totals_newVisits'].astype(float)\n",
    "tr['totals_transactions'] = tr['totals_transactions'].astype(float)\n",
    "tr['device_isMobile'] = tr['device_isMobile'].astype(bool)\n",
    "tr['trafficSource_isTrueDirect'] = tr['trafficSource_isTrueDirect'].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace all empty fields with NaN\n",
    "Nulls = ['(not set)', 'not available in demo dataset', '(not provided)', \n",
    "         'unknown.unknown', '/', 'Not Socially Engaged']\n",
    "for null in Nulls:    \n",
    "    tr.replace(null, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['totals_totalTransactionRevenue'] = tr['totals_totalTransactionRevenue'].astype(float)\n",
    "tr['totals_totalTransactionRevenue'].fillna(0, inplace=True)\n",
    "target = tr['totals_totalTransactionRevenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "tr[\"date\"] = pd.to_datetime(tr[\"date\"], infer_datetime_format=True, format=\"%Y%m%d\")\n",
    "def getTimeFramewithFeatures(tr, k=1):\n",
    "\n",
    "    tf = tr.loc[(tr['date'] >= min(tr['date']) + timedelta(days=168*(k-1))) \n",
    "              & (tr['date'] < min(tr['date']) + timedelta(days=168*k))]\n",
    "\n",
    "    tf_fvid = set(tr.loc[(tr['date'] >= min(tr['date']) + timedelta(days=168*k + 46 )) \n",
    "                       & (tr['date'] < min(tr['date']) + timedelta(days=168*k + 46 + 62))]['fullVisitorId'])\n",
    "\n",
    "    tf_returned = tf[tf['fullVisitorId'].isin(tf_fvid)]\n",
    "    \n",
    "    tf_tst = tr[tr['fullVisitorId'].isin(set(tf_returned['fullVisitorId']))\n",
    "             & (tr['date'] >= min(tr['date']) + timedelta(days=168*k + 46))\n",
    "             & (tr['date'] < min(tr['date']) + timedelta(days=168*k + 46 + 62))]\n",
    "    \n",
    "    tf_target = tf_tst.groupby('fullVisitorId')[['totals_totalTransactionRevenue']].sum().apply(np.log1p, axis=1).reset_index()\n",
    "    tf_target['ret'] = 1\n",
    "    tf_target.rename(columns={'totals_totalTransactionRevenue': 'target'}, inplace=True)\n",
    "    \n",
    "    tf_nonret = pd.DataFrame()\n",
    "    tf_nonret['fullVisitorId'] = list(set(tf['fullVisitorId']) - tf_fvid)    \n",
    "    tf_nonret['target'] = 0\n",
    "    tf_nonret['ret'] = 0\n",
    "    \n",
    "    tf_target = pd.concat([tf_target, tf_nonret], axis=0).reset_index(drop=True)\n",
    "    # len(set(tf['fullVisitorId'])), len(set(tf_target['fullVisitorId']))\n",
    "    tf_maxdate = max(tf['date'])\n",
    "    tf_mindate = min(tf['date'])\n",
    "\n",
    "    tf = tf.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork_networkDomain': {'networkDomain': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_city': {'city': lambda x: x.dropna().max()},\n",
    "            'device_operatingSystem': {'operatingSystem': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_metro': {'metro': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_region': {'region': lambda x: x.dropna().max()},\n",
    "            'channelGrouping': {'channelGrouping': lambda x: x.dropna().max()},\n",
    "            'trafficSource_referralPath': {'referralPath': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_country': {'country': lambda x: x.dropna().max()},\n",
    "            'trafficSource_source': {'source': lambda x: x.dropna().max()},\n",
    "            'trafficSource_medium': {'medium': lambda x: x.dropna().max()},\n",
    "            'trafficSource_keyword': {'keyword': lambda x: x.dropna().max()},\n",
    "            'device_browser':  {'browser': lambda x: x.dropna().max()},\n",
    "            'trafficSource_adwordsClickInfo.gclId': {'gclId': lambda x: x.dropna().max()},\n",
    "            'device_deviceCategory': {'deviceCategory': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_continent': {'continent': lambda x: x.dropna().max()},\n",
    "            'totals_timeOnSite': {'timeOnSite_sum': lambda x: x.dropna().sum(),\n",
    "                                  'timeOnSite_min': lambda x: x.dropna().min(), \n",
    "                                  'timeOnSite_max': lambda x: x.dropna().max(),\n",
    "                                  'timeOnSite_mean': lambda x: x.dropna().mean()},\n",
    "            'totals_pageviews': {'pageviews_sum': lambda x: x.dropna().sum(),\n",
    "                                 'pageviews_min': lambda x: x.dropna().min(), \n",
    "                                 'pageviews_max': lambda x: x.dropna().max(),\n",
    "                                 'pageviews_mean': lambda x: x.dropna().mean()},\n",
    "            'totals_hits': {'hits_sum': lambda x: x.dropna().sum(), \n",
    "                            'hits_min': lambda x: x.dropna().min(), \n",
    "                            'hits_max': lambda x: x.dropna().max(), \n",
    "                            'hits_mean': lambda x: x.dropna().mean()},\n",
    "            'visitStartTime': {'visitStartTime_counts': lambda x: x.dropna().count()},\n",
    "            'totals_sessionQualityDim': {'sessionQualityDim': lambda x: x.dropna().max()},\n",
    "            'trafficSource_isTrueDirect': {'isTrueDirect': lambda x: x.dropna().max()},\n",
    "            'totals_newVisits': {'newVisits_max': lambda x: x.dropna().max()},\n",
    "            'device_isMobile': {'isMobile': lambda x: x.dropna().max()},\n",
    "            'visitNumber': {'visitNumber_max' : lambda x: x.dropna().max()}, \n",
    "            'totals_totalTransactionRevenue':  {'totalTransactionRevenue_sum':  lambda x:x.dropna().sum()},\n",
    "            'totals_transactions' : {'transactions' : lambda x:x.dropna().sum()},\n",
    "            'date': {'first_ses_from_the_period_start': lambda x: x.dropna().min() - tf_mindate,\n",
    "                     'last_ses_from_the_period_end': lambda x: tf_maxdate - x.dropna().max(),\n",
    "                     'interval_dates': lambda x: x.dropna().max() - x.dropna().min(),\n",
    "                     'unqiue_date_num': lambda x: len(set(x.dropna())) },            \n",
    "                    })\n",
    "\n",
    "    tf.columns = tf.columns.droplevel()\n",
    "\n",
    "    tf = pd.merge(tf, tf_target, left_on='fullVisitorId', right_on='fullVisitorId')\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get 1st train part\n",
      "Get 2st train part\n",
      "Get 3st train part\n",
      "Get 4st train part\n"
     ]
    }
   ],
   "source": [
    "###Getting parts of train-set\n",
    "print('Get 1st train part')\n",
    "tr1 = getTimeFramewithFeatures(tr, k=1)\n",
    "tr1.to_pickle(PATH/'tr1_clean')\n",
    "\n",
    "print('Get 2st train part')\n",
    "tr2 = getTimeFramewithFeatures(tr, k=2)\n",
    "tr2.to_pickle(PATH/'tr2_clean')\n",
    "\n",
    "print('Get 3st train part')\n",
    "tr3 = getTimeFramewithFeatures(tr, k=3)\n",
    "tr3.to_pickle(PATH/'tr3_clean')\n",
    "\n",
    "print('Get 4st train part')\n",
    "tr4 = getTimeFramewithFeatures(tr, k=4)\n",
    "tr4.to_pickle(PATH/'tr4_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((377186, 42), (288869, 42), (385318, 42), (366202, 42))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr1.shape, tr2.shape, tr3.shape, tr4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get test\n"
     ]
    }
   ],
   "source": [
    "### Construction of the test-set (by analogy as train-set)\n",
    "print('Get test')\n",
    "tr5 = tr[tr['date'] >= pd.to_datetime(20180501, infer_datetime_format=True, format=\"%Y%m%d\")]\n",
    "tr5_maxdate = max(tr5['date'])\n",
    "tr5_mindate = min(tr5['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/generic.py:1315: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tr5 = tr5.groupby('fullVisitorId').agg({\n",
    "            'geoNetwork_networkDomain': {'networkDomain': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_city': {'city': lambda x: x.dropna().max()},\n",
    "            'device_operatingSystem': {'operatingSystem': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_metro': {'metro': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_region': {'region': lambda x: x.dropna().max()},\n",
    "            'channelGrouping': {'channelGrouping': lambda x: x.dropna().max()},\n",
    "            'trafficSource_referralPath': {'referralPath': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_country': {'country': lambda x: x.dropna().max()},\n",
    "            'trafficSource_source': {'source': lambda x: x.dropna().max()},\n",
    "            'trafficSource_medium': {'medium': lambda x: x.dropna().max()},\n",
    "            'trafficSource_keyword': {'keyword': lambda x: x.dropna().max()},\n",
    "            'device_browser':  {'browser': lambda x: x.dropna().max()},\n",
    "            'trafficSource_adwordsClickInfo.gclId': {'gclId': lambda x: x.dropna().max()},\n",
    "            'device_deviceCategory': {'deviceCategory': lambda x: x.dropna().max()},\n",
    "            'geoNetwork_continent': {'continent': lambda x: x.dropna().max()},\n",
    "            'totals_timeOnSite': {'timeOnSite_sum': lambda x: x.dropna().sum(),\n",
    "                                  'timeOnSite_min': lambda x: x.dropna().min(), \n",
    "                                  'timeOnSite_max': lambda x: x.dropna().max(),\n",
    "                                  'timeOnSite_mean': lambda x: x.dropna().mean()},\n",
    "            'totals_pageviews': {'pageviews_sum': lambda x: x.dropna().sum(),\n",
    "                                 'pageviews_min': lambda x: x.dropna().min(), \n",
    "                                 'pageviews_max': lambda x: x.dropna().max(),\n",
    "                                 'pageviews_mean': lambda x: x.dropna().mean()},\n",
    "            'totals_hits': {'hits_sum': lambda x: x.dropna().sum(), \n",
    "                            'hits_min': lambda x: x.dropna().min(), \n",
    "                            'hits_max': lambda x: x.dropna().max(), \n",
    "                            'hits_mean': lambda x: x.dropna().mean()},\n",
    "            'visitStartTime': {'visitStartTime_counts': lambda x: x.dropna().count()},\n",
    "            'totals_sessionQualityDim': {'sessionQualityDim': lambda x: x.dropna().max()},\n",
    "            'trafficSource_isTrueDirect': {'isTrueDirect': lambda x: x.dropna().max()},\n",
    "            'totals_newVisits': {'newVisits_max': lambda x: x.dropna().max()},\n",
    "            'device_isMobile': {'isMobile': lambda x: x.dropna().max()},\n",
    "            'visitNumber': {'visitNumber_max' : lambda x: x.dropna().max()}, \n",
    "            'totals_totalTransactionRevenue':  {'totalTransactionRevenue_sum':  lambda x:x.dropna().sum()},\n",
    "            'totals_transactions' : {'transactions' : lambda x:x.dropna().sum()},\n",
    "            'date': {'first_ses_from_the_period_start': lambda x: x.dropna().min() - tf_mindate,\n",
    "                     'last_ses_from_the_period_end': lambda x: tf_maxdate - x.dropna().max(),\n",
    "                     'interval_dates': lambda x: x.dropna().max() - x.dropna().min(),\n",
    "                     'unqiue_date_num': lambda x: len(set(x.dropna())) },\n",
    "                    })\n",
    "tr5.columns = tr5.columns.droplevel()\n",
    "tr5['target'] = np.nan\n",
    "tr5['ret'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr5.to_pickle(PATH/'tr5_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all pieces and converting the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([tr1, tr2, tr3, tr4, tr5], axis=0, sort=False).reset_index(drop=True)\n",
    "train_all['interval_dates'] = train_all['interval_dates'].dt.days\n",
    "train_all['first_ses_from_the_period_start'] = train_all['first_ses_from_the_period_start'].dt.days\n",
    "train_all['last_ses_from_the_period_end'] = train_all['last_ses_from_the_period_end'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.to_pickle(PATH/'train_and_test_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering train and test from combined dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change objects to category type\n",
    "cat_train(train_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_all[train_all['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_all[train_all['target'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of 'isReturned' classficator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb2 = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"binary_logloss\",\n",
    "        \"max_leaves\": 256,\n",
    "        \"num_leaves\" : 15,\n",
    "        \"min_child_samples\" : 1,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"bagging_fraction\" : 0.9,\n",
    "        \"feature_fraction\" : 0.8,\n",
    "        \"bagging_frequency\" : 1           \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of 'how_much_returned_will_pay' regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lgb3 = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"max_leaves\": 256,\n",
    "        \"num_leaves\" : 9,\n",
    "        \"min_child_samples\" : 1,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"bagging_fraction\" : 0.9,\n",
    "        \"feature_fraction\" : 0.8,\n",
    "        \"bagging_frequency\" : 1      \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and predicton of models: Averaging of 10 [Classificator*Regressor] values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['target', 'ret', 'fullVisitorId']\n",
    "\n",
    "dtrain_all = lgb.Dataset(train.drop(target_cols, axis=1), label=train['ret'])\n",
    "\n",
    "dtrain_ret = lgb.Dataset(train.drop(target_cols, axis=1)[train['ret']==1], \n",
    "                         label=train['target'][train['ret']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_lgb_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predictions\n",
      "Interation number  0\n",
      "Interation number  1\n",
      "Interation number  2\n",
      "Interation number  3\n",
      "Interation number  4\n",
      "Interation number  5\n",
      "Interation number  6\n",
      "Interation number  7\n",
      "Interation number  8\n",
      "Interation number  9\n"
     ]
    }
   ],
   "source": [
    "print('Training and predictions')\n",
    "for i in range(10):\n",
    "    print('Interation number ', i)\n",
    "    lgb_model1 = lgb.train(params_lgb2, dtrain_all, num_boost_round=1200)\n",
    "    pr_lgb = lgb_model1.predict(test.drop(target_cols, axis=1))\n",
    "    \n",
    "    lgb_model2 = lgb.train(params_lgb3, dtrain_ret, num_boost_round=368)\n",
    "    pr_lgb_ret = lgb_model2.predict(test.drop(target_cols, axis=1))\n",
    "    \n",
    "    pr_lgb_sum = pr_lgb_sum + pr_lgb*pr_lgb_ret\n",
    "\n",
    "pr_final2 = pr_lgb_sum/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
